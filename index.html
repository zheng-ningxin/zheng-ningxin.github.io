<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="shortcut icon" href="favicon.ico" />
<link rel="bookmark" href="favicon.ico" type="image/x-icon"　/>
<title>Zheng, Ningxin (郑宁馨)</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="https://scholar.google.com/citations?user=ND2CeBkAAAAJ&hl=zh-CN">Google Scholar</a></div>
<div class="menu-item"><a href="https://github.com/zheng-ningxin">GitHub</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Zheng, Ningxin (郑宁馨) </h1>
</div>
<table class="imgtable"><tr><td>
<a href="https://zheng-ningxin.github.io/"><img src="photos/bio.jpeg" alt="alt text" width="160px" height="160px" /></a>&nbsp;</td>
<td align="left"><p>Research Software Development Engineer II,<br />
Microsoft Research,<br />
No. 701 Yunjin Road, <br />
Shanghai, China <br /> 
E-mail: <a href="mailto:NingxinZheng@sjtu.edu.cn">NingxinZheng@sjtu.edu.cn</a></p>
</td></tr></table>
<h2>About me</h2>
<p>I received a B.S. degree from the HuaZhong University of Science and Technology, in 2017, and an M.S. degree from Shanghai Jiao Tong University in 2020, advised by prof. Minyi Guo and Quan Chen. I am currently a research software development engineer II at Microsoft Research Shanghai. My research interests include (1) AI systems: model deployment optimization(speedup), sparsity optimization, etc. (2) Cloud computing: improving resource utilization by co-locating different jobs, and resource management in data centers (3)Model Compression.</p>
<h2>Research</h2>

<h3>System Publications</h3>
<ol> 
<li><p> <b>Ningxin Zheng</b>, Huiqiang Jiang, Quanlu Zhang, Zhenhua Han, Lingxiao Ma, Yuqing Yang, Fan Yang, Chengruidong Zhang, Lili Qiu, Mao Yang, Lidong Zhou, "PIT: Optimization of Dynamic Sparse Deep Learning Models via Permutation Invariant Transformation", <i>SOSP23</i> </p>
</li>
<li><p> Weihao Cui, Zhenhua Han, Lingji Ouyang, Yichuan Wang, <b>Ningxin Zheng</b>, Lingxiao Ma, Yuqing Yang, Fan Yang, Jilong Xue, Lili Qiu, Lidong Zhou, Quan Chen, Haisheng Tan, Minyi Guo, "Optimizing Dynamic Neural Networks with Brainstorm", <i>OSDI23</i> </p>
</li>
<li><p> Bin Lin, <b>Ningxin Zheng</b>, Shijie Cao, Lingxiao Ma, Quanlu Zhang, Yi Zhu, Ting Cao, Jilong Xue, Yuqing Yang, Fan Yang, "Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning", <i>MLSys23</i> [<a href="https://github.com/microsoft/SparTA/tree/nmsparse_artifact">code</a>]</p>
</li>
<li><p> <b>Ningxin Zheng</b>, Bin Lin, Quanlu Zhang, Lingxiao Ma, Yuqing Yang, Fan Yang, Yang Wang, Mao Yang, Lidong Zhou, "SparTA: Deep-Learning Model Sparsity via Tensor-with-Sparsity-Attribute", <i>OSDI22</i> [<a href="https://www.usenix.org/system/files/osdi22-zheng-ningxin.pdf">pdf</a>][<a href="https://github.com/microsoft/SparTA">code</a>]</p>
</li>
<li><p> Wei Zhang, Quan Chen, Kaihua Fu, <b>Ningxin Zheng</b>, Zhiyi Huang, Jingwen Leng, Minyi Guo, "Astraea: towards QoS-aware and resource-efficient multi-stage GPU services", <i>ASPLOS22</i>, [<a href="https://www.microsoft.com/en-us/research/uploads/prod/2022/07/asplos22.pdf">pdf</a>]</p>
</li>
<li><p> Kaihua Fu, Jiuchen Shi, Quan Chen, <b>Ningxin Zheng</b>, Wei Zhang, Deze Zeng, Minyi Guo, "QoS-Aware Irregular Collaborative Inference for Improving Throughput of DNN Services", <i>SC22</i>, [<a href="https://csdl-downloads.ieeecomputer.org/proceedings/sc/2022/5444/00/544400a993.pdf?Expires=1674011850&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jc2RsLWRvd25sb2Fkcy5pZWVlY29tcHV0ZXIub3JnL3Byb2NlZWRpbmdzL3NjLzIwMjIvNTQ0NC8wMC81NDQ0MDBhOTkzLnBkZiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY3NDAxMTg1MH19fV19&Signature=bxuZFz6Xp1Mh1O34OvQH1zjky73o~4gpgoPi2b8cbTTwMBnfjvM03Wbj0HP7qJ4yFj3SZcDXcZEdaJeX0RMSed~jFIF6Kf4pVrWrRS2r9c0QRkcTV-zx-HJaeGLI2tBgr6~UlPQwTbcWjMt2IH01C0vksy18zQj3ohT86iSHIVof2U~YRxED6-3ORfyyCv6teiwdvydhC4bueySgHOqEu28tqfRJiOyEznIDQ~RvTB6fAgBntQsNZecRV5lYnVKL~z8PH7y7iGbNkIWe2eL22Wg3LimjAhhwu4OnbwWGJRd0Pb6KPn-7jNv6VgZlHpE7VtYYfsMJ-LNPJAe-fv3Lew__&Key-Pair-Id=K12PMWTCQBDMDT">pdf</a>]</p>
</li>
<li><p> Wei Zhang, Kaihua Fu, <b>Ningxin Zheng</b>, Quan Chen, Chao Li, Wenli Zheng, Minyi Guo, "CHARM: Collaborative Host and Accelerator Resource Management for GPU Datacenters", <i>ICCD21</i>, [<a href="publication/charm.pdf">pdf</a>]</p>
</li>
<li><p> Weihao Cui, Han Zhao, Quan Chen, <b>Ningxin Zheng</b>, Jingwen Leng, Jieru Zhao, Zhuo Song, Tao Ma, Yong Yang, Chao Li, Minyi Guo, "Enable simultaneous DNN services based on deterministic operator overlap and precise latency prediction.", <i>SC21</i>, [<a href="https://www.microsoft.com/en-us/research/uploads/prod/2022/07/3458817.3476143.pdf">pdf</a>]</p>
</li>
<li><p> Li Lyna Zhang, Shihao Han, Jianyu Wei, <b>Ningxin Zheng</b>, Ting Cao, Yuqing Yang, Yunxin Liu, "nn-Meter: Towards Accurate Latency Prediction of Deep-Learning Model Inference on Diverse Edge Devices", <i>MobiSys21</i>, <b>Best Paper Award</b> && <b>SigMobile Research Highlight</b> [<a href="https://air.tsinghua.edu.cn/pdf/nn-Meter-Towards-Accurate-Latency-Prediction-of-Deep-Learning-Model-Inference-on-Diverse-Edge-Devices.pdf">pdf</a>][<a href="https://github.com/microsoft/nn-Meter">code</a>]</p>
</li>
<li><p> Wei Zhang, Quan Chen, <b>Ningxin Zheng</b>, Weihao Cui, Kaihua Fu, Minyi Guo, "Towards QoS-awareness and Improved Utilization of Spatial Multitasking GPUs", <i>TC21</i>, [<a href="publication/tc21.pdf">pdf</a>]</p>
</li>
<li><p> Wei Zhang, <b>Ningxin Zheng</b>, Quan Chen, Yong Yang, Zhuo Song, Tao Ma, Jingwen Leng, Minyi Guo, "URSA: Precise Capacity Planning and Fair Scheduling based on Low-level Statistics for Public Clouds", <i>ICPP20</i>, <b>Co-first Author</b> [<a href="https://jnamaral.github.io/icpp20/slides/Zhang_URSA.pdf">pdf</a>]</p>
</li>
<li><p> <b>Ningxin Zheng</b>, Quan Chen, Yong Yang, Jin Li, Wenli Zheng, Minyi Guo, "POSTER:Precise Capacity Planning for Database Public Clouds", <i>PACT19</i></p>
</li>
<li><p> <b>Ningxin Zheng</b>, Quan Chen, Chen Chen, Minyi Guo, "CLIBE: Precise Cluster-Level I/O Bandwidth Enforcement in Distributed File System", <i>HPCC18</i></p>
</li>
</ol>
<h3>Algorithm Publications</h3>
<ol> 
  <li><p> Li Lyna Zhang, Xudong Wang, Jiahang Xu, Quanlu Zhang, Yujing Wang, Yuqing Yang, <b>Ningxin Zheng</b>, Ting Cao, Mao Yang, "SpaceEvo: Hardware-Friendly Search Space Design for Efficient INT8 Inference", <i>ICCV23</i> [<a href="publication/tmm.pdf">pdf</a>]</p>
  </li>
  <li><p> Xinyu Liu, Houwen Peng, <b>Ningxin Zheng</b>, Yuqing Yang, Han Hu, Yixuan Yuan, "EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention", <i>CVPR23</i> [<a href="publication/efficient_vit.pdf">pdf</a>]</p>
  </li>
  <li><p> Jun Xiao, Xinyang Jiang, <b>Ningxin Zheng</b>, Huan Yang, Yifan Yang, Yuqing Yang, Dongsheng Li, Kin-Man Lam, "Online Video Super-Resolution with Convolutional Kernel Bypass Graft", <i>IEEE Transaction on Multimedia 22</i> [<a href="publication/tmm.pdf">pdf</a>]</p>
  </li>
</ol>

<p><a href="https://scholar.google.com/citations?user=ND2CeBkAAAAJ&hl=zh-CN">Full list of publications in Google Scholar</a>.</p>

<h2>Projects</h2>
<ol>
<li><p><a href="https://github.com/microsoft/nni">NNI</a></p></li>
<ul>
<li><p> NNI is a very popular Deep learning framework (over 10k stars) including Neural Architecture Search(NAS), Model Compression, Hyperparameter Tuning, and Feature engineering. As the DNN models grow significantly, they are inevitably becoming sparse. Model Compression is an essential step before model deployment. As a core contributor, I designed and developed the automatic deployment process of the compressed model("Speedup" Module in NNI). "Speedup" can infer the sparsity of the whole model and generate the corresponding optimized faster model automatically. It simplifies the model deployment progress. </p>
</li>
</ul>
<li><p><a href="https://github.com/microsoft/SparTA.git">SparTA</a></p></li>
<ul>
<li><p>SparTA is an extensible sparse framework based on Pytorch that supports different kinds of sparsity scenarios. It is the open-source implementation of our OSDI paper(SparTA). It contains lots of easy-to-use sparse modules that can be easily used in many scenarios such as large model training and sparse model inference. Compared to other sparse libraries, SparTA has achieved better performance and covered more application scenarios.</p>
</li>
</ul>

<li><p>Performance Optimization for High frequency trading system</p></li>
  <ul>
  <li><p> China foreign exchange trade system(CFETS) receives a large number of transaction requests every second, therefore it has extremely high requirements for performance. Constrained by the complex transaction logic, it is difficult to improve system throughput through task parallelism. In order to improve the performance of the system, we analyzed the performance bottleneck of the system through "Perf", split the transaction logic into three parts, and parallelized them in the pipeline. Finally, the end-to-end throughput improves by around 30%. </p>
  </li>
  </ul>

  <li><p><a href="https://github.com/zheng-ningxin/CLIBE">CLIBE</a></p></li>
    <ul>
    <li><p>CLIBE provides a precise cluster-level I/O bandwidth enforcement mechanism for distributed file systems. The big data file system is widely used in different scenarios and such distributed file system(DFS) is usually shared by multiple tenants/jobs. Such sharing may lead to uncontrollable  I/O bandwidth interference. The Quality-of-Service(QoS) of high-priority jobs may be violated due to I/O bandwidth interference. CLIBE allows the user to allocate a cluter-level I/O bandwidth quota for each jod and ensures that the I/O bandwidth consumed by the target task in the entire cluster is lower than the allocated quota.</p>
    </li>
    </ul>
    
</ol>

<h2>Patent</h2>
<p>Precise Capacity Planning and Fair Scheduling based on Low-level Statistics for Public Clouds, Alibaba</p>
<p>SparTA: Deep-Learning Model Sparsity via Tensor-with-Sparsity-Attribute, Microsoft</p>

<h2>Education</h2>
<p>Master degree, Computer Science and Technology, Shanghai Jiao Tong University, 09.2017~03.2020</p>
<p>Bachelor degree, Computer Science and Technology, Huazhong University of Science and Technology, 09.2013~06.2017</p>

<h3>Competitions and Awards</h3>
<ol>

<li><p><b>Mobisys Best Paper</b>, 2021</p>
</li>
<li><p>SigMobile Research Highlight, 2021</p>
</li>
<li><p>Outstanding Graduate of Shanghai Jiao Tong University, 2020</p>
</li>
<li><p>Scholarship of DongShi DongFang of Shanghai Jiao Tong University, 2019</p>
</li>
<li><p>Bronze medal of Intel Parallel performance Optimization Competition, 2017</p>
</li>
<li><p>First-Class Scholarship of Shanghai Jiao Tong University, 2017</p>
</li>
<li><p>Outstanding student of Huazhong University of Science and Technology, 2015</p>
</li>
</ol>

<h2>Work experience</h2>
<ol>
<li><p>Microsoft Research</p></li>
<ul>
<li><p>Research Software Development Engineer, Shanghai System Group, 2020.03~current</p>
</li>
</ul>
<li><p>Alibaba Cloud</p></li>
<ul>
<li><p>Software Developer Intern, Linux Kernel Group, 2018.03~2019.07</p>
</li>
</ul>
</ol>

</td>
</tr>
</table>
</body>
</html>
