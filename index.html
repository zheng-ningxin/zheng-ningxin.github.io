<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="shortcut icon" href="favicon.ico" />
<link rel="bookmark" href="favicon.ico" type="image/x-icon"　/>
<title>Zheng, Ningxin (郑宁馨)</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="https://scholar.google.com/citations?user=ND2CeBkAAAAJ&hl=zh-CN">Google Scholar</a></div>
<div class="menu-item"><a href="https://github.com/zheng-ningxin">GitHub</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Zheng, Ningxin (郑宁馨) </h1>
</div>
<table class="imgtable"><tr><td>
<a href="https://zheng-ningxin.github.io/"><img src="photos/bio.jpeg" alt="alt text" width="160px" height="160px" /></a>&nbsp;</td>
<td align="left"><p>
ByteDance AML,<br />
Caohejing, Xuhui District, <br />
Shanghai, China <br /> 
E-mail: <a href="mailto:NingxinZheng@sjtu.edu.cn">NingxinZheng@sjtu.edu.cn</a></p>
</td></tr></table>
<h2>About me</h2>
<p>I received a B.S. from HuaZhong University of Science and Technology, and an M.S. from Shanghai Jiao Tong University, under the guidance of Professors Minyi Guo and Quan Chen. Presently, I contribute to ByteDance's AML team, focusing on enhancing the efficiency and scalability of Large Language Model (LLM) training. My research pursuits encompass AI systems, with an emphasis on LLM training optimization, model deployment (inference), and sparsity; cloud computing, aiming to boost resource utilization through job co-location and data center resource management; and model compression.</p>
<h2>Research</h2>

<h3>SelectedSystem Publications</h3>
<ol> 
  <li><p> Shulai Zhang, <b>Ningxin Zheng</b>, Haibin Lin, Ziheng Jiang, Wenlei Bao, Chengquan Jiang, Qi Hou, Weihao Cui, Size Zheng, Li-Wen Chang, Quan Chen, Xin Liu, "COMET: Fine-grained Computation-communication Overlapping for Mixture-of-Experts", <b>Corresponding Author</b>, <i>MLSys25</i> </p>
  </li>
  <li><p> Size Zheng, Jin Fang, Xuegui Zheng, Qi Hou, Wenlei Bao, <b>Ningxin Zheng</b>, Ziheng Jiang, Dongyang Wang, Jianxi Ye, Haibin Lin, Li-Wen Chang, Xin Liu, "TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives", <i>MLSys25</i> </p>
  </li>
  <li><p> Lei Wang, Lingxiao Ma, Shijie Cao, Quanlu Zhang, Jilong Xue, Yining Shi, <b>Ningxin Zheng</b>, Ziming Miao, Fan Yang, Ting Cao, Yuqing Yang, Mao Yang, "Bitter: Enabling Efficient Low-Precision Deep Learning Computing through Hardware-aware Tensor Transformation", <i>OSDI24</i> </p>
  </li>
  <li><p> <b>Ningxin Zheng</b>, Huiqiang Jiang, Quanlu Zhang, Zhenhua Han, Lingxiao Ma, Yuqing Yang, Fan Yang, Chengruidong Zhang, Lili Qiu, Mao Yang, Lidong Zhou, "PIT: Optimization of Dynamic Sparse Deep Learning Models via Permutation Invariant Transformation", <i>SOSP23</i> </p>
  </li>
  <li><p> Weihao Cui, Zhenhua Han, Lingji Ouyang, Yichuan Wang, <b>Ningxin Zheng</b>, Lingxiao Ma, Yuqing Yang, Fan Yang, Jilong Xue, Lili Qiu, Lidong Zhou, Quan Chen, Haisheng Tan, Minyi Guo, "Optimizing Dynamic Neural Networks with Brainstorm", <i>OSDI23</i> </p>
  </li>
  <li><p> Lei Wang, Lingxiao Ma, Shijie Cao, <b>Ningxin Zheng</b>, Quanlu Zhang, Jilong Xue, Ziming Miao, Ting Cao, Yuqing Yang, "LADDER: Efficient Tensor Compilation on Customized Data Format", <i>OSDI23 POSTER Session</i> </p>
  </li>
  <li><p> Bin Lin, <b>Ningxin Zheng</b>, Shijie Cao, Lingxiao Ma, Quanlu Zhang, Yi Zhu, Ting Cao, Jilong Xue, Yuqing Yang, Fan Yang, "Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning", <b>Co-first Author</b>, <i>MLSys23</i> [<a href="https://github.com/microsoft/SparTA/tree/nmsparse_artifact">code</a>]</p>
  </li>
  <li><p> <b>Ningxin Zheng</b>, Bin Lin, Quanlu Zhang, Lingxiao Ma, Yuqing Yang, Fan Yang, Yang Wang, Mao Yang, Lidong Zhou, "SparTA: Deep-Learning Model Sparsity via Tensor-with-Sparsity-Attribute", <i>OSDI22</i> [<a href="https://www.usenix.org/system/files/osdi22-zheng-ningxin.pdf">pdf</a>][<a href="https://github.com/microsoft/SparTA">code</a>]</p>
  </li>
  <li><p> Wei Zhang, Quan Chen, Kaihua Fu, <b>Ningxin Zheng</b>, Zhiyi Huang, Jingwen Leng, Minyi Guo, "Astraea: towards QoS-aware and resource-efficient multi-stage GPU services", <i>ASPLOS22</i>, [<a href="https://www.microsoft.com/en-us/research/uploads/prod/2022/07/asplos22.pdf">pdf</a>]</p>
  </li>
  <li><p> Kaihua Fu, Jiuchen Shi, Quan Chen, <b>Ningxin Zheng</b>, Wei Zhang, Deze Zeng, Minyi Guo, "QoS-Aware Irregular Collaborative Inference for Improving Throughput of DNN Services", <i>SC22</i>, [<a href="https://csdl-downloads.ieeecomputer.org/proceedings/sc/2022/5444/00/544400a993.pdf?Expires=1674011850&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jc2RsLWRvd25sb2Fkcy5pZWVlY29tcHV0ZXIub3JnL3Byb2NlZWRpbmdzL3NjLzIwMjIvNTQ0NC8wMC81NDQ0MDBhOTkzLnBkZiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY3NDAxMTg1MH19fV19&Signature=bxuZFz6Xp1Mh1O34OvQH1zjky73o~4gpgoPi2b8cbTTwMBnfjvM03Wbj0HP7qJ4yFj3SZcDXcZEdaJeX0RMSed~jFIF6Kf4pVrWrRS2r9c0QRkcTV-zx-HJaeGLI2tBgr6~UlPQwTbcWjMt2IH01C0vksy18zQj3ohT86iSHIVof2U~YRxED6-3ORfyyCv6teiwdvydhC4bueySgHOqEu28tqfRJiOyEznIDQ~RvTB6fAgBntQsNZecRV5lYnVKL~z8PH7y7iGbNkIWe2eL22Wg3LimjAhhwu4OnbwWGJRd0Pb6KPn-7jNv6VgZlHpE7VtYYfsMJ-LNPJAe-fv3Lew__&Key-Pair-Id=K12PMWTCQBDMDT">pdf</a>]</p>
  </li>
  <li><p> Wei Zhang, Kaihua Fu, <b>Ningxin Zheng</b>, Quan Chen, Chao Li, Wenli Zheng, Minyi Guo, "CHARM: Collaborative Host and Accelerator Resource Management for GPU Datacenters", <i>ICCD21</i>, [<a href="publication/charm.pdf">pdf</a>]</p>
  </li>
  <li><p> Weihao Cui, Han Zhao, Quan Chen, <b>Ningxin Zheng</b>, Jingwen Leng, Jieru Zhao, Zhuo Song, Tao Ma, Yong Yang, Chao Li, Minyi Guo, "Enable simultaneous DNN services based on deterministic operator overlap and precise latency prediction.", <i>SC21</i>, [<a href="https://www.microsoft.com/en-us/research/uploads/prod/2022/07/3458817.3476143.pdf">pdf</a>]</p>
  </li>
  <li><p> Li Lyna Zhang, Shihao Han, Jianyu Wei, <b>Ningxin Zheng</b>, Ting Cao, Yuqing Yang, Yunxin Liu, "nn-Meter: Towards Accurate Latency Prediction of Deep-Learning Model Inference on Diverse Edge Devices", <i>MobiSys21</i>, <b>Best Paper Award</b> && <b>SigMobile Research Highlight</b> [<a href="https://air.tsinghua.edu.cn/pdf/nn-Meter-Towards-Accurate-Latency-Prediction-of-Deep-Learning-Model-Inference-on-Diverse-Edge-Devices.pdf">pdf</a>][<a href="https://github.com/microsoft/nn-Meter">code</a>]</p>
  </li>
  <li><p> Wei Zhang, Quan Chen, <b>Ningxin Zheng</b>, Weihao Cui, Kaihua Fu, Minyi Guo, "Towards QoS-awareness and Improved Utilization of Spatial Multitasking GPUs", <i>TC21</i>, [<a href="publication/tc21.pdf">pdf</a>]</p>
  </li>
  <li><p> Wei Zhang, <b>Ningxin Zheng</b>, Quan Chen, Yong Yang, Zhuo Song, Tao Ma, Jingwen Leng, Minyi Guo, "URSA: Precise Capacity Planning and Fair Scheduling based on Low-level Statistics for Public Clouds", <i>ICPP20</i>, <b>Co-first Author</b> [<a href="https://jnamaral.github.io/icpp20/slides/Zhang_URSA.pdf">pdf</a>]</p>
  </li>
  <li><p> <b>Ningxin Zheng</b>, Quan Chen, Yong Yang, Jin Li, Wenli Zheng, Minyi Guo, "POSTER:Precise Capacity Planning for Database Public Clouds", <i>PACT19</i></p>
  </li>
  <li><p> <b>Ningxin Zheng</b>, Quan Chen, Chen Chen, Minyi Guo, "CLIBE: Precise Cluster-Level I/O Bandwidth Enforcement in Distributed File System", <i>HPCC18</i></p>
  </li>
</ol>
<h3>SelectedAlgorithm Publications</h3>
<ol> 
  <li><p> Li Lyna Zhang, Xudong Wang, Jiahang Xu, Quanlu Zhang, Yujing Wang, Yuqing Yang, <b>Ningxin Zheng</b>, Ting Cao, Mao Yang, "SpaceEvo: Hardware-Friendly Search Space Design for Efficient INT8 Inference", <i>ICCV23</i> [<a href="publication/tmm.pdf">pdf</a>]</p>
  </li>
  <li><p> Xinyu Liu, Houwen Peng, <b>Ningxin Zheng</b>, Yuqing Yang, Han Hu, Yixuan Yuan, "EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention", <i>CVPR23</i> [<a href="publication/efficient_vit.pdf">pdf</a>]</p>
  </li>
  <li><p> Jun Xiao, Xinyang Jiang, <b>Ningxin Zheng</b>, Huan Yang, Yifan Yang, Yuqing Yang, Dongsheng Li, Kin-Man Lam, "Online Video Super-Resolution with Convolutional Kernel Bypass Graft", <i>IEEE Transaction on Multimedia 22</i> [<a href="publication/tmm.pdf">pdf</a>]</p>
  </li>
</ol>

<p><a href="https://scholar.google.com/citations?user=ND2CeBkAAAAJ&hl=zh-CN">Full list of publications in Google Scholar</a>.</p>

<h2>Projects</h2>
<ol>
<li><p><a href="https://github.com/bytedance/flux">Flux</a></p></li>
<ul>
<li><p>Flux is a communication-overlapping library for dense/MoE models on GPUs, providing high-performance and pluggable kernels to support various parallelisms in model training/inference. Flux's efficient kernels are compatible with Pytorch and can be integrated into existing frameworks easily, supporting various Nvidia GPU architectures and data types. Flux has been adopted in the production environment of clusters with ten-thousand-scale of GPUs, achieving savings of millions of GPU hours.</p>
</li>
</ul>
<li><p><a href="https://github.com/microsoft/nni">NNI</a></p></li>
<ul>
<li><p> NNI is a very popular Deep learning framework (over 10k stars) including Neural Architecture Search(NAS), Model Compression, Hyperparameter Tuning, and Feature engineering. As the DNN models grow significantly, they are inevitably becoming sparse. Model Compression is an essential step before model deployment. As a core contributor, I designed and developed the automatic deployment process of the compressed model("Speedup" Module in NNI). "Speedup" can infer the sparsity of the whole model and generate the corresponding optimized faster model automatically. It simplifies the model deployment progress. </p>
</li>
</ul>
<li><p><a href="https://github.com/microsoft/SparTA.git">SparTA</a></p></li>
<ul>
<li><p>SparTA is an extensible sparse framework based on Pytorch that supports different kinds of sparsity scenarios. It is the open-source implementation of our OSDI paper(SparTA). It contains lots of easy-to-use sparse modules that can be easily used in many scenarios such as large model training and sparse model inference. Compared to other sparse libraries, SparTA has achieved better performance and covered more application scenarios.</p>
</li>
</ul>

<li><p>Performance Optimization for High frequency trading system</p></li>
  <ul>
  <li><p> China foreign exchange trade system(CFETS) receives a large number of transaction requests every second, therefore it has extremely high requirements for performance. Constrained by the complex transaction logic, it is difficult to improve system throughput through task parallelism. In order to improve the performance of the system, we analyzed the performance bottleneck of the system through "Perf", split the transaction logic into three parts, and parallelized them in the pipeline. Finally, the end-to-end throughput improves by around 30%. </p>
  </li>
  </ul>

  <li><p><a href="https://github.com/zheng-ningxin/CLIBE">CLIBE</a></p></li>
    <ul>
    <li><p>CLIBE provides a precise cluster-level I/O bandwidth enforcement mechanism for distributed file systems. The big data file system is widely used in different scenarios and such distributed file system(DFS) is usually shared by multiple tenants/jobs. Such sharing may lead to uncontrollable  I/O bandwidth interference. The Quality-of-Service(QoS) of high-priority jobs may be violated due to I/O bandwidth interference. CLIBE allows the user to allocate a cluter-level I/O bandwidth quota for each jod and ensures that the I/O bandwidth consumed by the target task in the entire cluster is lower than the allocated quota.</p>
    </li>
    </ul>
    
</ol>

<h2>Patent</h2>
<p>Precise Capacity Planning and Fair Scheduling based on Low-level Statistics for Public Clouds, Alibaba</p>
<p>SparTA: Deep-Learning Model Sparsity via Tensor-with-Sparsity-Attribute, Microsoft</p>
<p>Optimization of Dynamic Sparse Deep Learning Models via Permutation Invariant Transformation</p>

<h2>Education</h2>
<p>Master degree, Computer Science and Technology, Shanghai Jiao Tong University</p>
<p>Bachelor degree, Computer Science and Technology, Huazhong University of Science and Technology</p>

<h3>Competitions and Awards</h3>
<ol>

<li><p><b>Mobisys Best Paper</b></p>
</li>
<li><p>SigMobile Research Highlight</p>
</li>
<li><p>Outstanding Graduate of Shanghai Jiao Tong University</p>
</li>
<li><p>Scholarship of DongShi DongFang of Shanghai Jiao Tong University</p>
</li>
<li><p>Bronze medal of Intel Parallel performance Optimization Competition</p>
</li>
<li><p>First-Class Scholarship of Shanghai Jiao Tong University</p>
</li>
<li><p>Outstanding student of Huazhong University of Science and Technology</p>
</li>
</ol>

<h2>Work experience</h2>
<ol>
<li><p>ByteDance AML</p></li>
<ul>
<li><p>2023.09~Currnet</p>
</li>
</ul>
<li><p>Microsoft Research</p></li>
<ul>
<li><p>Research Software Development Engineer, Shanghai System Group, 2020.03~2023.9</p>
</li>
</ul>
<li><p>Alibaba Cloud</p></li>
<ul>
<li><p>Software Developer Intern, Linux Kernel Group, 2018.03~2019.07</p>
</li>
</ul>
</ol>

</td>
</tr>
</table>
</body>
</html>
